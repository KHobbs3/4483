# Interpreting results:
- [benjamini-hochberg](https://bioc.ism.ac.jp/packages/2.14/bioc/vignettes/DESeq2/inst/doc/beginner.pdf)) adjusts p-value to account for type I errors (false positives)
when errors occur by chance
- calculates the fraction of false positives given a p-value and population size
  - e.g. if p = 0.01 and a population size of 4000, 40 of 121 positive results or 33% will be false
  - adjustment calculation: arbitrary threshold of "acceptable" false positives (e.g. 10%), all results with adjp < 10% are significant
## dada2
- <2% of the reads were found to be chimeric. Indicates a good wet lab protocol!


## Biplots
- poor proportion of variance explained (as prinicipal component) do not neccessarily accurately represent the data
- any PC less than 80% is considered poor
- loadings are centered around zero =
- mothur and dada2 biplots both show considerable overlap between subgroups, showcasing no distinguishing taxonomic relationships
- PC1 of mothur is slightly higher (10%) than of dada2 output (7%) although both are too low to ensure accurate representation of results
  - loadings are OTUs/features
  - position of samples depends on position of loadings -- loadings will drive sample variation
  - distance between loadings is informative:
    - overlapping of points implies a relationship
    - distance between samples is NOT informative:
    - because PCAs are multi-dimensional, overlapping points may appear to be similar
        in the plotted 2 dimensions but upon looking from a 3rd dimension may not show correlation
            - pairwise analysis will reveal where samples differ*


## Effect plots
- difference within versus between groups
- benjamini-hochberg adjusted p
- effect size = difference between/within
Bayesian Stats:
- FPs and FNs are just a recall of true FPs and FNs
- false discovery rate depends on quality of dataset, low effect size = more true results than false present so no matter the algorithm used the results are likely to be TRUE.
Therefore FPs may still be meaningful.
- positive predictive value is low(?)
- in sparse datasets FPs are rare
- small datasets means larger effect sizes will be observed (skewed) even though they may not be significant in a larger population, therefore they're not as different as they may seem.

## [DESeq2](https://bioc.ism.ac.jp/packages/2.14/bioc/vignettes/DESeq2/inst/doc/beginner.pdf)
- *base mean:* average of normalized counts/size factors across all samples
- *log2foldchange:* effect size estimate
- *log2foldchange standard error (lfcSE):* standard error associated with effect size estimate
- *hypothesis test:* Wald test
  - null = no effect or correlation of condition to SV (difference between test and control is due to experimental variability)
  - results show p-values where p = 0.01
    - NA p-values assigned to zero counts or extreme outliers, in which the SV was excluded from the hypothesis test
    - benjamini-hochberg adjusted p ('padj' column)
  - [Wald test:](http://www.statisticshowto.com/wald-test/)
    - chi-squared test or 'likelihood ratio'
    - removes zeros
    - similar to t-test for large sample sizes
    - way to find out if variables contribute to 'model':
      1. find MLE: maximum likelihood estimator
      2. find expected Fisher information
      3. evaluate Fisher info at MLE

- significance depends on (1) LFC and (2) dispersion:
        (1) shrinkage estimation of log fold changes (LFCs): low count values are "shrunken" towards zero to avoid skewing ranked LFCs (would appear too large)
            -  *MA plots:*
              - each dot = SV
              - significant SVs are coloured red (padj < 0.1)
        (2) dispersion = variability within groups
            - *Dispersion plots:*
              - each dot = SV dispersion estimate
              - red line = fitted mean-dispersion relationship
              - final estimates = estimates shrunk to the red line
              - dispersion outliers are not fitted to line and appear outside of main "cloud"

- volcano plots:
    - low p-values (highly significant data) appear towards the top of the plot


## [filtR](https://github.com/bjoris33/filtR/blob/master/R/filtR_function.R)
*To find technical errors*
    - clr mean difference: technical errors supposedly to have low abundance relative to its OTU "associates".
      Low cutoff means more OTUs will be removed as technical errors... if this value is low and there are still no technical errors than the method of
    obtaining or processing the data must be pretty experimentally sound!

    - rho cutoff: measure of beta-associate between two OTUs.
     Each individual OTU, for the most part, should lack association with other OTUs because they are independent entities
     Similarily, a high rho cutoff will keep more OTUs.

    - final1 table: after filtering by rho cutoff
    - final5 table: after filtering by clr mean difference cutoff
    - filtR_table: counts table with technical errors removed

---

## [Unifrac](https://joey711.github.io/phyloseq-demo/unifrac.html)
- Weighted UniFrac - which does take into account differences in abundance of taxa between samples, but takes longer to calculate; and
- Unweighted UniFrac - which only considers the presence/absence of taxa between sample pairs.
